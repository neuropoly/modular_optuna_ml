Study Configuration
===================

.. _study-config-walkthrough:

Study Configuration Set-Up
----------------------------

Same as before, we will start by defining how we want runs using this study configuration to be labelled:

.. code-block:: json

    {
        "label": "tutorial_study",
        ...
    }

We should also specify the random number generator seed as well, to ensure repeated runs of the study yield the same results. This value must be an integer value, or omitted entirely (in which case each run will be, effectively, completely random!):

.. code-block:: json

    {
        ...
        "random_seed": 72164,
        ...
    }

Now we can start defining the design of the study. This is split into three different components:

* ``no_replicates``: The number of testing splits that will be generated by MOOP. The higher this is, the more robust the distribution of model performance will be, at the cost MOOP analyses taking longer.
* ``no_trials``: The number of trials Optuna will use to tune the hyper-parameters of the model we defined in the model configuration. The higher this is, the more "tuned" these parameters will be to the training data, at the expense of MOOP taking longer, and an increase possibility of the models overfitting to each training set.
* ``no_crosses``: The number of train-validate splits that will be generated per-replicate, and how many evaluations of the loss function will be used during hyper-parameter tuning. The higher this is, the more stable the tuning of the model's performance will be, at the expense of a longer run-time as well.

A more in-depth discussion of these parameters, and how they relate to how MOOP managed an Optuna study, can be found in **TODO**.

Each of these can be any integer greater than 1, and can be specified in the configuration file like so:

.. code-block:: json

    {
        ...
        "no_replicates": 10,
        "no_trials": 10,
        "no_crosses": 10,
        ...
    }

Now we need to define what the model should try and predict, and how its performance at doing so should be measured. These are defined with the ``target`` and ``objective`` parameters, respectively.

Referring back to our original dataset, the ``target`` would be the 'baz' column. As this is classification task, we'll use the classic measure of performance as well; ``log_loss``. Lets add these to the config:

.. code-block:: json

    {
        ...
        "target": "baz",
        "objective": "log_loss"
        ...
    }

.. note::

    Currently MOOP only supports supervised learning analyses, and as such a ``target`` column must always be specified in the study configuration file. This is subject to change, in which case the configuration file will only need to provide an ``objective`` parameter in that context.

Finally, we want to define where we want the results of MOOP to be stored. This will *always* be a SQLite Database (usually denoted with the ``.db`` extension), and we can explicitly define where we want it to be placed with the ``output_path`` param:

.. code-block:: json

    {
        ...
        "output_path": "output/moop_results.db"
    }

Voila! You now have viable study configuration file. If you followed everything up to this point, it should look something like this:

.. code-block:: json

    {
        "label": "tutorial_study",
        "random_seed": 72164,
        "no_replicates": 10,
        "no_trials": 10,
        "no_crosses": 10,
        "target": "baz",
        "objective": "log_loss",
        "output_path": "output/moop_results.db"
    }

Performance and Structure Tracking
----------------------------------

In its current state, MOOP will only report the objectives value across trials and replicates, as evaluated on the cross-validated train-validate splits. If all you care about is tracking the model's performance in this way, you can stop here!

Assuming you specified some model hyperparameters as tunable in the model configuration file, however, you probably want to track how those change across trials and replicates as well. To let MOOP know this, we can add a single param to our configuration file:

.. code-block:: json

    {
        ...
        "track_params": true
    }

You might also want to measure the model's performance in different ways; ``log_loss`` is useful and all, but not very intuitive in some cases. The metric defined on the ``objective`` is also the average performance of model from that trial applied to the validation split of each cross, **not** on the testing set! If we want measures of performance at that stage, we need to leverage the ``metric`` param.

The ``metric`` parameter is unique in that it is a dictionary of three separate lists, each of which can contain any number of to-be-evaluated metrics. Each list corresponds to different stage of the analysis where a metric can be evaluated:

* ``train``: Metrics placed here will be evaluated at the train-validate split (during cross-validation), after the model has been trained on the training split of the data. These metrics are assessed based on the model's performance on the validation split of the data.
* ``validate``: Metrics placed here will be evaluated at the testing split (once per replicate), after the model has been trained on the train-validate split of the data. These metrics are assessed based on the model's performance on the train-validation set.
* ``test``: Metrics placed here will be evaluated at the testing split (once per replicate), after the model has been trained on the train-validate split of the data. These metrics are assessed based on the model's performance on the testing set.

.. warning::

    The ``train`` parameter differs from the ``validate`` and ``test`` alternatives, as (to access the train split) MOOP must evaluate these metrics during the cross-validation step of its analysis. As a result, any metrics you define here will be evaluated (and recorded) once per cross, resulting in multiple columns in the final result database, equal to the ``no_crosses`` you specify. Naturally, this can bloat the size of the output file quite substantially, so keep this in mind when deciding what metrics you want measured and where!

For an example of how to utilize this, lets track the balanced accuracy of our model on both the train-validation and test sets of each split. To do so, we modify the study configuration like so:

.. code-block:: json

    {
        ...
        "metrics": {
            "validate": [
                "balanced_accuracy"
            ],
            "test": [
                "balanced_accuracy"
            ]
        }
    }


Note that any metric that can be defined as an objective is also valid as measured metric here as well. For example, lets extend our configuration file to also record ``log_loss`` during cross-validation and testing:

.. code-block:: json

    {
        ...
        "metrics": {
            "train": [
                "log_loss"
            ]
            ...
            "test": [
                "balanced_accuracy",
                "log_loss"
            ]
        }
    }

Voila! Your study configuration file will now track more than just the ``log_loss`` at validation.
